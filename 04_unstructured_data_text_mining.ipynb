{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_unstructured_data_text_mining.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrybrew/sma-health/blob/master/04_unstructured_data_text_mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awnEsM81kACc",
        "colab_type": "text"
      },
      "source": [
        "# **Coronavirus Vaccine**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g9GloB_kz_d",
        "colab_type": "text"
      },
      "source": [
        "Coronavirus disease 2019 (COVID-19) is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). It was first identified in December 2019 in Wuhan, Hubei, China, and has resulted in an ongoing pandemic. The first confirmed case has been traced back to 17 November 2019 in Hubei.As of 6 August 2020, more than 18.7 million cases have been reported across 188 countries and territories, resulting in more than 706,000 deaths. More than 11.3 million people have recovered.\n",
        "\n",
        "Until now, a vaccine for this disease has not been found. However, lately a lot of parties have started to raise rumors about the issue of vaccine findings.\n",
        "\n",
        "Most of the people today share opinions and information about COVID-19 vaccine through various social media, including Twitter. Today we will analyze the content of conversations between Twitter users regarding the keyword \"vaccine\" and \"vaccines\" in English."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTFmncw78173",
        "colab_type": "text"
      },
      "source": [
        "We use text mining technique to analyze this textual data. Text mining is a process of exploring sizeable textual data and find patterns. Text Mining process the text itself. Finding frequency counts of words, length of the sentence, presence/absence of specific words is known as text mining.\n",
        "\n",
        "Natural language processing is one of the components of text mining. NLP helps identified sentiment, finding entities in the sentence, and category of blog/article. Text mining is preprocessed data for text analytics. In Text Analytics, statistical and machine learning algorithm used to classify information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2afMRC54AJkM",
        "colab_type": "text"
      },
      "source": [
        "## **A. Text Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i14aOA1Tm0x0",
        "colab_type": "text"
      },
      "source": [
        "Text preprocessing is traditionally an important step for natural language processing (NLP) tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4gYhrbtC4rl",
        "colab_type": "text"
      },
      "source": [
        "***Import Library***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umR5WapdlW_C",
        "colab_type": "text"
      },
      "source": [
        "We need to import some libraries first. Here are the libraries we need to import."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4rKM6x-C97u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import library for Text Analytics\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhqJUzVl_DDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Libraries for Data Manipulation\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI9l6zjVDFVt",
        "colab_type": "text"
      },
      "source": [
        "***Import Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64Md_yy4lwh-",
        "colab_type": "text"
      },
      "source": [
        "Then, import our demonetization tweets dataset into this notebook using Pandas library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duG0Aw--96b6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Data\n",
        "tweets=pd.read_csv('https://raw.githubusercontent.com/andrybrew/sma-health/master/data/vaccine.csv')\n",
        "tweets.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfbBE4wVm2vC",
        "colab_type": "text"
      },
      "source": [
        "***Select Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMYa8Kw4t4kf",
        "colab_type": "text"
      },
      "source": [
        "We will use only text(tweets) data in this text mining modeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUS3DkJj_dsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select Only Text Column\n",
        "text = tweets[['text']]\n",
        "text.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mydcGr66m8tp",
        "colab_type": "text"
      },
      "source": [
        "***Clean the Dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRlihIkS_5I7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Text Cleaning Function using Regex\n",
        "import re\n",
        "\n",
        "def  clean_text(df, text_field):\n",
        "    df[text_field] = df[text_field].str.lower()\n",
        "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
        "    # remove numbers\n",
        "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
        "    return df\n",
        "\n",
        "# Apply to the data\n",
        "text_clean = clean_text(text, 'text')\n",
        "text_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gclqDLHICQ0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Stopword\n",
        "import nltk.corpus\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Apply Stopword to the dataframe\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "text_clean['nostopword'] = text_clean['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "text_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xar0blD5mXsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Punkt\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "text_clean['tokenize'] = text_clean['nostopword'].apply(lambda x: word_tokenize(x))\n",
        "text_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBF-5Jxsm77r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Stemmer\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Create Stemmer Function\n",
        "def word_stemmer(text):\n",
        "    stem_text = [PorterStemmer().stem(i) for i in text]\n",
        "    return stem_text\n",
        "\n",
        "# Apply to the dataframe\n",
        "text_clean['stemming'] = text_clean['tokenize'].apply(lambda x: word_stemmer(x))\n",
        "text_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YnfZR9EnnsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Wordnet Library\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Create Lematization Funtion\n",
        "def word_lemmatizer(text):\n",
        "    lem_text = [WordNetLemmatizer().lemmatize(i, pos='v') for i in text]\n",
        "    return lem_text\n",
        "\n",
        "# Apply to a dataframe\n",
        "text_clean['lemmatization'] = text_clean['tokenize'].apply(lambda x: word_lemmatizer(x))\n",
        "text_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-ka-0qPowMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to a New Dataframe\n",
        "text_preprocessed = text_clean['lemmatization'].str.join(\",\") \n",
        "text_preprocessed = text_preprocessed.str.replace(',', ' ', regex=False)\n",
        "text_preprocessed = pd.DataFrame(text_preprocessed)\n",
        "text_preprocessed.rename(columns={'lemmatization': 'text'}, inplace = True)\n",
        "text_preprocessed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNDdgELqtgt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save as CSV\n",
        "text_preprocessed.to_csv('text_preprocessed_vaccine.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_K6wk7YAap_",
        "colab_type": "text"
      },
      "source": [
        "## **B. Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuSavPiKkSQV",
        "colab_type": "text"
      },
      "source": [
        "Text classification is the process of assigning tags or categories to text according to its content. It’s one of the fundamental tasks in Natural Language Processing (NLP) with broad applications such as sentiment analysis, topic labeling, spam detection, and intent detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rrtw8ARM7H4b"
      },
      "source": [
        "***Import Library***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRusYti47L66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import library for Text Analytics\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rH-MTmU7UDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Libraries for Data Manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFtFP_hB7XJ_",
        "colab_type": "text"
      },
      "source": [
        "***Modeling Sentiment Analysis***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4D8U5jI-w1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Module\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
        "\n",
        "# Sentiment Analysis\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "listy = [] \n",
        "for index, row in text_preprocessed.iterrows():\n",
        "  ss = sid.polarity_scores(row['text'])\n",
        "  listy.append(ss)\n",
        "  \n",
        "se = pd.Series(listy)\n",
        "text_preprocessed['polarity'] = se.values\n",
        "display(text_preprocessed.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stJP2xoR_PM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pie Chart Visualization\n",
        "labels = ['negative', 'neutral', 'positive']\n",
        "sizes  = [ss['neg'], ss['neu'], ss['pos']]\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
        "plt.axis('equal') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zItUWz6kyTgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save as CSV\n",
        "text_preprocessed.to_csv('sentiment_vaccine.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDiwU2gV_eRY",
        "colab_type": "text"
      },
      "source": [
        "## **C. Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC8fK2kzfW4j",
        "colab_type": "text"
      },
      "source": [
        "Topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear equally in both. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG2t_7n6fawH",
        "colab_type": "text"
      },
      "source": [
        "***Install Library, Import Libraries, and Import Modules***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxM8QfGWRt0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install Library\n",
        "! pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LTY4ilBSCdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Libraries\n",
        "import nltk\n",
        "import os\n",
        "import numpy as np, pyLDAvis, pyLDAvis.sklearn; pyLDAvis.enable_notebook()\n",
        "\n",
        "# Import Modules\n",
        "from __future__ import print_function \n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSkNONYpSEXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone Library and Data from Github\n",
        "! git clone https://github.com/dianrdn/tm\n",
        "\n",
        "# Set Data Directory\n",
        "os.chdir('tm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVEv1nVgG9K5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Stop Words\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Import Data\n",
        "text_preprocessed = 'text_preprocessed_vaccine.csv'\n",
        "\n",
        "# Load Tweets Data\n",
        "import MyLib as TS\n",
        "Tweets = TS.LoadTxt(text_preprocessed) \n",
        "print('Total loaded tweets = {0}'.format(len(Tweets)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdDudz3lgWy4",
        "colab_type": "text"
      },
      "source": [
        "***Set Number of Topics, Top Topics, Top Words***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK4J4At2gRwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_topics = 4\n",
        "top_topics = 4\n",
        "top_words = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AL5bu8Tld7S",
        "colab_type": "text"
      },
      "source": [
        "***Feature Extraction***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w25SADrrPf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature Extraction\n",
        "count_vector = CountVectorizer(lowercase = True, token_pattern = r'\\b[a-zA-Z]{3,}\\b') \n",
        "dtm_tf = count_vector.fit_transform(Tweets)\n",
        "tf_terms = count_vector.get_feature_names()\n",
        "del Tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aS5jGV0up2p",
        "colab_type": "text"
      },
      "source": [
        "***Show Topic***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcMlhTzUuyrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Topic Search Function\n",
        "lda_tf = LatentDirichletAllocation(n_components=n_topics, learning_method='online', random_state=0).fit(dtm_tf)\n",
        "\n",
        "# Show Topics\n",
        "vsm_topics = lda_tf.transform(dtm_tf); doc_topic =  [a.argmax()+1 for a in tqdm(vsm_topics)] # topic of docs\n",
        "print('In total there are {0} major topics, distributed as follows'.format(len(set(doc_topic))))\n",
        "plt.hist(np.array(doc_topic), alpha=0.5); plt.show()\n",
        "print('Printing top {0} Topics, with top {1} Words:'.format(top_topics, top_words))\n",
        "TS.print_Topics(lda_tf, tf_terms, top_topics, top_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRG28QyXYPwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Interactively visualizing the Topics, please ignore the Warnings\n",
        "# Wait few minutes and then hover the Mouse over the Topics to Explore\n",
        "pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, count_vector) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPyCX7-2NB_W",
        "colab_type": "text"
      },
      "source": [
        "## **D. Text Network Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXKbtyB8KIZP",
        "colab_type": "text"
      },
      "source": [
        "Though network analysis is most often used to describe relationships between people, some of the early pioneers of network analysis realized that it could also be applied to represent relationships between words. For example, one can represent a corpus of documents as a network where each node is a document, and the thickness or strength of the edges between them describes similarities between the words used in any two documents. Or, one can create a textnetwork where individual words are the nodes, and the edges between them describe the regularity with which they co-occur in documents.\n",
        "\n",
        "There are multiple advantages to a network-based approach to automated text analysis. Just as clusters of social connections can help explain a range of outcomes, understanding patterns of connections between words helps identify their meaning in a more precise manner.Second, text networks can be built out of documents of any length, whereas topic models function poorly on short texts such as social media messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQhsmBwSKxl3",
        "colab_type": "text"
      },
      "source": [
        "In this prcatice we will use NetworkX. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. You can see the full documentation of NetworkX HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vroZ-J0c2QXR",
        "colab_type": "text"
      },
      "source": [
        "Here we construct a text network based on conversations about 'Demonetization in India'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajIRyYeAW9lq",
        "colab_type": "text"
      },
      "source": [
        "**Install & Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv2l0EPlXXLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import nltk\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from nltk import bigrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from random import seed\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb8Q4cX8IqIv",
        "colab_type": "text"
      },
      "source": [
        "**Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9Saf4RFIout",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Data\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/andrybrew/sma-health/master/data/text_preprocessed_vaccine_short.csv', sep = ',')\n",
        "\n",
        "# Show Data\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrOmikMd1hky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to String\n",
        "df['text'] = df['text'].fillna('').apply(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4Sl8MO_y5h-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select Text\n",
        "text = df['text']\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBY920BBAhKW",
        "colab_type": "text"
      },
      "source": [
        "### **Preparing Adjacency Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQslFdEkzOpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize\n",
        "text_data = [word_tokenize(i) for i in text]\n",
        "print(text_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAMLUtpR13Kb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Fuction to show co occurrence\n",
        "def generate_co_occurrence_matrix(corpus):\n",
        "    vocab = set(corpus)\n",
        "    vocab = list(vocab)\n",
        "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
        " \n",
        "    # Create bigrams from all words in corpus\n",
        "    bi_grams = list(bigrams(corpus))\n",
        " \n",
        "    # Frequency distribution of bigrams ((word1, word2), num_occurrences)\n",
        "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
        " \n",
        "    # Initialise co-occurrence matrix\n",
        "    # co_occurrence_matrix[current][previous]\n",
        "    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
        " \n",
        "    # Loop through the bigrams taking the current and previous word,\n",
        "    # and the number of occurrences of the bigram.\n",
        "    for bigram in bigram_freq:\n",
        "        current = bigram[0][1]\n",
        "        previous = bigram[0][0]\n",
        "        count = bigram[1]\n",
        "        pos_current = vocab_index[current]\n",
        "        pos_previous = vocab_index[previous]\n",
        "        co_occurrence_matrix[pos_current][pos_previous] = count\n",
        "    co_occurrence_matrix = np.matrix(co_occurrence_matrix)\n",
        " \n",
        "    # return the matrix and the index\n",
        "    return co_occurrence_matrix, vocab_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT7Vqips15iS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create one list using many lists\n",
        "data = list(itertools.chain.from_iterable(text_data))\n",
        "matrix, vocab_index = generate_co_occurrence_matrix(data)\n",
        " \n",
        " \n",
        "data_matrix = pd.DataFrame(matrix, index=vocab_index,\n",
        "                             columns=vocab_index)\n",
        "\n",
        "# Show Adjacency Matrix\n",
        "data_matrix.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IHqMGi96bvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_matrix.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9MLNkEJBFZx",
        "colab_type": "text"
      },
      "source": [
        "### **Constructing Text Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq8InbQzGf5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct Network form Data Matrix\n",
        "G = nx.from_pandas_adjacency(data_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9XH2HTw07CZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make A Random Sample of the node\n",
        "import random\n",
        "k = 500\n",
        "\n",
        "sampled_nodes = random.sample(G.nodes, k)\n",
        "sampled_graph = G.subgraph(sampled_nodes)\n",
        "\n",
        "# Visualize the Network\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,10))\n",
        "nx.draw(sampled_graph, with_labels=True, \n",
        "        node_color='skyblue', node_size=600, \n",
        "        arrowstyle='->',arrowsize=20, edge_color='r',\n",
        "        font_size=7,\n",
        "        pos=nx.kamada_kawai_layout(sampled_graph))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XibgWnJBoNL",
        "colab_type": "text"
      },
      "source": [
        "### **Network Metrics and Measurement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47fv2rgnKShO",
        "colab_type": "text"
      },
      "source": [
        "**Centrality Measurement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eonE6ffXKfqU",
        "colab_type": "text"
      },
      "source": [
        "In graph theory and network analysis, indicators of centrality identify the most important vertices within a graph. Applications include identifying the most influential person(s) in a social network, key infrastructure nodes in the Internet or urban networks, and super-spreaders of disease. Centrality concepts were first developed in social network analysis, and many of the terms used to measure centrality reflect their sociological origin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG3tJEkqKVPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Degree Centrality\n",
        "degree = nx.degree_centrality(sampled_graph)\n",
        "\n",
        "# Sorted from the Highest\n",
        "sorted(nx.degree(sampled_graph), key=lambda x: x[1], reverse=True)[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88vr0guJOVuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Betweenness Centrality\n",
        "betweenness = nx.betweenness_centrality(sampled_graph)\n",
        "\n",
        "# Sorted from the Highest\n",
        "sorted(nx.betweenness_centrality(sampled_graph, normalized=True).items(), key=lambda x:x[1], reverse=True)[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRKPlKYsPGMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Closeness Centrality\n",
        "closeness = nx.closeness_centrality(sampled_graph)\n",
        "\n",
        "# Sorted from the Highest\n",
        "sorted(nx.closeness_centrality(sampled_graph).items(), key=lambda x:x[1], reverse=True)[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XWM0uLSPdSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Eigenvector Centrality\n",
        "eigenvector = nx.eigenvector_centrality(sampled_graph)\n",
        "\n",
        "# Sorted from the Highest\n",
        "sorted(nx.eigenvector_centrality(sampled_graph).items(), key=lambda x:x[1], reverse=True)[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMxWj0m7De5J",
        "colab_type": "text"
      },
      "source": [
        "***Visualize Network based on Centrality Measurement***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whzLeVNKbPj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set Degree Dictionary\n",
        "d = dict(degree)\n",
        "\n",
        "# Visualize the Network\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,10))\n",
        "nx.draw(sampled_graph, with_labels=True, \n",
        "        node_color='skyblue', nodelist=d.keys(),\n",
        "        node_size=[v * 50000 for v in d.values()], \n",
        "        arrowstyle='->',arrowsize=20, edge_color='r',\n",
        "        font_size=8,\n",
        "        pos=nx.kamada_kawai_layout(sampled_graph))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9j8PrQWwUA7",
        "colab_type": "text"
      },
      "source": [
        "**Network Topology Measurement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLNTtHEkRWK-",
        "colab_type": "text"
      },
      "source": [
        "The configuration, or topology, of a network is key to determining its performance. Network topology is the way a network is arranged, including the physical or logical description of how links and nodes are set up to relate to each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLuKZD791wb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show Number of Nodes\n",
        "nx.number_of_nodes(sampled_graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHT180js1lHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show Number of Edges\n",
        "nx.number_of_edges(sampled_graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6W83y_4xpVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show Graph Density\n",
        "nx.density(sampled_graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52AuWdV1BtUZ",
        "colab_type": "text"
      },
      "source": [
        "### **Community Detection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qIbaO55RvCz",
        "colab_type": "text"
      },
      "source": [
        "Community detection is a fundamental problem in dividing text (modelled as nodes in a social graph) with certain word connections into densely knitted and highly related groups with each group well separated from different group members."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPq8nalnLduK",
        "colab_type": "text"
      },
      "source": [
        "**Modularity Community**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCv3Y88JeH1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Module\n",
        "from networkx.algorithms.community import greedy_modularity_communities\n",
        "\n",
        "# Modularity Community Detection\n",
        "communities_m = sorted(greedy_modularity_communities(sampled_graph), key=len, reverse=True)\n",
        "communities_m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwxt7KhbHK-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set Node Community Function\n",
        "def set_node_community(sampled_graph, communities_m):\n",
        "      '''Add community to node attributes'''\n",
        "      for c, v_c in enumerate(communities_m):\n",
        "        for v in v_c:\n",
        "          # Add 1 to save 0 for external edges\n",
        "          sampled_graph.nodes[v]['community'] = c + 1      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujM90pkvH3ME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set Colour Function\n",
        "def get_color(i, r_off=1, g_off=1, b_off=1):\n",
        "     '''Assign a color to a vertex.'''\n",
        "     r0, g0, b0 = 0, 0, 0\n",
        "     n = 16\n",
        "     low, high = 0.1, 0.9\n",
        "     span = high - low\n",
        "     r = low + span * (((i + r_off) * 3) % n) / (n - 1)\n",
        "     g = low + span * (((i + g_off) * 5) % n) / (n - 1)\n",
        "     b = low + span * (((i + b_off) * 7) % n) / (n - 1)\n",
        "     return (r, g, b) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9uXID-6IJfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set Node Communities\n",
        "community = set_node_community(sampled_graph, communities_m)\n",
        "\n",
        "# Set Node Color\n",
        "node_color = [get_color(sampled_graph.nodes[v]['community']) for v in sampled_graph.nodes]\n",
        "\n",
        "# Visualize the Network\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,10))\n",
        "nx.draw(sampled_graph, with_labels=True, \n",
        "        node_color = node_color, node_size=600, \n",
        "        arrowstyle='->',arrowsize=20, edge_color='r',\n",
        "        font_size=7, map = plt.get_cmap('jet'),\n",
        "        pos=nx.kamada_kawai_layout(sampled_graph))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87CZow4LNG0c",
        "colab_type": "text"
      },
      "source": [
        "## **E. Word Cloud**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwoMONXRViNu",
        "colab_type": "text"
      },
      "source": [
        "A word cloud is a collection, or cluster, of words depicted in different sizes. The bigger and bolder the word appears, the more often it’s mentioned within a given text and the more important it is.\n",
        "\n",
        "Also known as tag clouds or text clouds, these are ideal ways to pull out the most pertinent parts of textual data, from blog posts to databases. They can also help business users compare and contrast two different pieces of text to find the wording similarities between the two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNOv2JnJNK4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Libraries\n",
        "import wordcloud\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t19w8uySBXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Modules\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "\n",
        "# Build Word Cloud\n",
        "text = \" \".join(review for review in df.text)\n",
        "cloud = WordCloud(background_color='white').generate(text)\n",
        "plt.figure(figsize=(7, 7), facecolor=None)\n",
        "plt.imshow(cloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdRf95s9QUmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save File\n",
        "cloud.to_file(\"wordcloud.png\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}